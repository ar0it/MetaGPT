from typing import List, Dict, Set, Type, Any, Union
from pydantic import BaseModel
from deprecated import deprecated
from contextlib import contextmanager
import json
from datetime import datetime
import sys
from io import StringIO
from docstring_parser import parse
from ome_types._autogenerated.ome_2016_06 import XMLAnnotation, OME, StructuredAnnotations
import re
import ome_types
from metagpt.utils.DataClasses import Sample, Dataset
import importlib
import os
from typing import Optional
import ast
import xml.etree.ElementTree as ET
from ome_types import from_xml, to_xml, to_dict
import jsonpatch
import time
import tiktoken


def render_cell_output(output_path):
    """
    Load the captured output from a file and render it.

    Parameters:
    output_path (str): Path to the output file where the cell output is saved.
    """
    try:
        # Read the output file
        with open(output_path, 'r', encoding='utf-8') as f:
            output_data = json.load(f)
        
        # Print the captured stdout and stderr
        if 'stdout' in output_data:
            print(output_data['stdout'], end='')
        if 'stderr' in output_data:
            print(output_data['stderr'], end='', file=sys.stderr)
        
        print(f"\nCell output loaded from {output_path}")
    
    except Exception as e:
        print(f"An error occurred: {e}")

class Tee:
    def __init__(self, *streams):
        self.streams = streams
    
    def write(self, data):
        for stream in self.streams:
            stream.write(data)
    
    def flush(self):
        for stream in self.streams:
            stream.flush()

@contextmanager
def save_and_stream_output(output_path=f"out/jupyter_cell_outputs/cell_output_{datetime.now().isoformat()}_.json"):
    """
    Context manager to capture the output of a code block, save it to a file,
    and print it to the console in real-time.

    Parameters:
    output_path (str): Path to the output file where the cell output will be saved.
    """
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    stdout_buffer = StringIO()
    stderr_buffer = StringIO()

    sys.stdout = Tee(sys.stdout, stdout_buffer)
    sys.stderr = Tee(sys.stderr, stderr_buffer)
    
    try:
        yield
    finally:
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        
        output_data = {
            'stdout': stdout_buffer.getvalue(),
            'stderr': stderr_buffer.getvalue()
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=4)
        
        print(f"\nCell output saved to {output_path}")


def from_dict(ome_dict, state:BaseModel=None) -> OME:
    """
    Convert a dictionary to an OME object.
    """
    def set_attributes(obj, data):
        for key, value in data.items():
            if isinstance(value, dict):
                attr = getattr(obj, key, None)
                if attr is not None:
                    set_attributes(attr, value)
                else:
                    setattr(obj, key, value)
            elif isinstance(value, list):
                # Assume the list items are dictionaries that need similar treatment
                existing_list = getattr(obj, key, [])
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        if i < len(existing_list):
                            set_attributes(existing_list[i], item)
                        else:
                            # Handle the case where the list item does not already exist
                            existing_list.append(item)
                    else:
                        existing_list.append(item)
                setattr(obj, key, existing_list)
            else:
                setattr(obj, key, value)
    if state is None:
        state = OME()
    set_attributes(state, ome_dict)
    return state

def _init_logger():
    """This is so that Javabridge doesn't spill out a lot of DEBUG messages
    during runtime.
    From CellProfiler/python-bioformats.
    """
    import javabridge as jb
    rootLoggerName = jb.get_static_field("org/slf4j/Logger",
                                         "ROOT_LOGGER_NAME",
                                         "Ljava/lang/String;")

    rootLogger = jb.static_call("org/slf4j/LoggerFactory",
                                "getLogger",
                                "(Ljava/lang/String;)Lorg/slf4j/Logger;",
                                rootLoggerName)

    logLevel = jb.get_static_field("ch/qos/logback/classic/Level",
                                   "ERROR",
                                   "Lch/qos/logback/classic/Level;")

    jb.call(rootLogger,
            "setLevel",
            "(Lch/qos/logback/classic/Level;)V",
            logLevel)
    
@deprecated()
def collect_dependencies(model: Type[BaseModel], known_models: Dict[str, Type[BaseModel]], collected: Dict[str, Type[BaseModel]]):
    """
    Function to identify dependencies and collect all models
    """
    if model.__name__ in collected:
        return
    collected[model.__name__] = model
    for field in model.__fields__.values():
        field_type = field.annotation
        if hasattr(field_type, '__fields__'):  # If the field is a Pydantic model
            collect_dependencies(field_type, known_models, collected)
        elif hasattr(field_type, '__origin__') and field_type.__origin__ is list:
            item_type = field_type.__args__[0]
            if hasattr(item_type, '__fields__'):
                collect_dependencies(item_type, known_models, collected)

@deprecated()
def get_dependencies(model: Type[BaseModel], known_models: Dict[str, Type[BaseModel]]) -> Set[str]:
    """
    Function to identify dependencies for sorting
    """
    dependencies = set()
    for field in model.model_fields.values():
        field_type = field.annotation
        if hasattr(field_type, '__fields__'):  # If the field is a Pydantic model
            dependencies.add(field_type.__name__)
            dependencies.update(get_dependencies(field_type, known_models))
        elif hasattr(field_type, '__origin__') and field_type.__origin__ is list:
            item_type = field_type.__args__[0]
            if hasattr(item_type, '__fields__'):
                dependencies.add(item_type.__name__)
                dependencies.update(get_dependencies(item_type, known_models))
    return dependencies

@deprecated()
def sort_models_by_dependencies(root_model: Type[BaseModel]) -> List[Type[BaseModel]]:
    """
    Function to sort models by dependencies
    """
    known_models = {model.__name__: model for model in globals().values() if isinstance(model, type) and issubclass(model, BaseModel)}
    collected_models = {}
    collect_dependencies(root_model, known_models, collected_models)
    
    dependency_graph: Dict[str, Set[str]] = {
        model.__name__: get_dependencies(model, known_models) for model in collected_models.values()
    }
    
    sorted_models = []
    while dependency_graph:
        # Find models with no dependencies or dependencies that are already resolved
        resolvable_models = {name for name, deps in dependency_graph.items() if not deps - set(m.__name__ for m in sorted_models)}
        if not resolvable_models:
            raise ValueError("Circular dependency detected or unresolved dependency found")
        
        # Add resolvable models to the sorted list
        for model_name in resolvable_models:
            sorted_models.append(collected_models[model_name])
            dependency_graph.pop(model_name)
    
    return sorted_models


def openai_schema(cls: BaseModel, additional_ignored_keywords:list[str]=[]) -> Dict[str, Any]:
    """
    Return the schema in the format of OpenAI's schema as jsonschema

    Note:
        It's important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.

    Returns:
        dict: A dictionary in the format of OpenAI's schema as jsonschema
    """
    schema = cls.model_json_schema()
    docstring = parse(cls.__doc__ or "")
    
    def clean_description(desc: Union[str, Any]) -> str:
        if not isinstance(desc, str):
            return str(desc)
        cleaned = re.sub(r'\s+', ' ', desc)
        return cleaned.strip()

    def resolve_ref(ref: str, definitions: Dict[str, Any]) -> Dict[str, Any]:
        if not ref.startswith('#/$defs/'):
            return {'type': 'object', 'description': f'Unresolved reference: {ref}'}
        def_name = ref.split('/')[-1]
        return definitions.get(def_name, {'type': 'object', 'description': f'Missing definition: {def_name}'})

    def flatten_schema(prop: Any, definitions: Dict[str, Any]) -> Any:
        if not isinstance(prop, dict):
            return prop

        flattened = {}
        unnecessary_keys = [] + additional_ignored_keywords  # Add any other keys you want to exclude
        for key, value in prop.items():
            if key not in unnecessary_keys:
                if key == '$ref':
                    flattened.update(flatten_schema(resolve_ref(value, definitions), definitions))
                elif key == "required" and (value == True or value == False):
                    continue
                elif key == 'description' and type(value) == str:
                    flattened[key] = clean_description(value)
                elif isinstance(value, dict):
                    flattened[key] = flatten_schema(value, definitions)
                elif isinstance(value, list):
                    flattened[key] = [flatten_schema(item, definitions) for item in value]
                else:
                    flattened[key] = value

        return flattened

    # Extract $defs if present
    definitions = schema.pop('$defs', {})

    # Flatten the main schema
    flattened_schema = flatten_schema(schema, definitions)

    # Add flattened definitions to properties
    if 'properties' not in flattened_schema:
        flattened_schema['properties'] = {}
    flattened_schema['properties']['definitions'] = {
        'type': 'object',
        'properties': {k: flatten_schema(v, definitions) for k, v in definitions.items()}
    }

    # Add descriptions from docstring
    for param in docstring.params:
        if param.arg_name in flattened_schema.get('properties', {}) and param.description:
            flattened_schema['properties'][param.arg_name]['description'] = clean_description(param.description)

    # Combine short_description and long_description for a more complete description
    full_description = docstring.short_description or ""
    if docstring.long_description:
        full_description += " " + docstring.long_description if full_description else docstring.long_description
    
    description = clean_description(full_description) if full_description else f"Correctly extracted `{cls.__name__}` with all the required parameters with correct types"

    return {
        "type": "function",
        "function":{
            "name": schema.get('title', cls.__name__),
            "description": description,
            "parameters": {
                "type": "object",
                "required": [],
                "properties": flattened_schema["properties"]
            }
        }
    }
    
@deprecated()
def dict_to_xml_annotation(value:dict) -> XMLAnnotation:
    """
    Convert a dictionary to an XMLAnnotation object.
    value: dict - The dictionary to be converted to an XMLAnnotation object. It requires the key 'xml_annotations' which is a dictionary of key-value pairs.
    """
    fun = lambda k,v : {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                        'text': '',
                        'children': [{'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Key',
                                      'text': k}, {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Value',
                                                   'text': v}]}
    if value["annotations"] is list:
        value["annotations"] = value["annotations"][0]
        
    test_value = {"any_elements": [fun(k, v) for k, v in value["annotations"].items()]}
    return XMLAnnotation(value=test_value)


def dict_to_xml_annotation(value: dict) -> XMLAnnotation:
    """
    Convert a dictionary to an XMLAnnotation object, handling nested dictionaries.
    
    value: dict - The dictionary to be converted to an XMLAnnotation object. 
    It requires the key 'annotations' which is a dictionary of key-value pairs.
    """
    def create_element(key, value):
        if isinstance(value, dict):
            return {
                'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                'text': '',
                'children': [
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Key', 'text': key},
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Value', 'text': ''},
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                     'text': '',
                     'children': [create_element(k, v) for k, v in value.items()]}
                ]
            }
        elif isinstance(value, list):
            return {
                'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                'text': '',
                'children': [
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Key', 'text': key},
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Value', 'text': ''},
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                     'text': '',
                     'children': [create_element(f"{key}_{i}", v) for i, v in enumerate(value)]}
                ]
            }
        else:
            return {
                'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}OriginalMetadata',
                'text': '',
                'children': [
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Key', 'text': key},
                    {'qname': '{http://www.openmicroscopy.org/Schemas/OME/2016-06}Value', 'text': str(value)}
                ]
            }

    if isinstance(value.get("annotations"), list):
        value["annotations"] = value["annotations"][0]
    
    if not isinstance(value.get("annotations"), dict):
        try:
            value["annotations"] = ast.literal_eval(value.get("annotations"))

        except Exception as e:
            print("The 'annotations' key must be a dictionary" + e)

    test_value = {"any_elements": [create_element(k, v) for k, v in value["annotations"].items()]}
    return XMLAnnotation(value=test_value)


def merge_xml_annotation(annot: Dict[str, Any], ome: str = None) -> Optional[str]:
    """
    Merge the annotation section with the OME XML.

    Args:
        ome (Optional[str]): The OME XML string.
        annot (Optional[Dict[str, Any]]): The annotation dictionary.

    Returns:
        Optional[str]: The merged XML string, or None if inputs are invalid.
    """
    print("Merging structured annotations")
    if not ome:
        print("No OME XML provided as strarting point for merge")
        ome = OME()
        ome = ome_types.to_xml(ome)

    # remove all structured annotations from the start point
    ome = from_xml(ome)
    ome.structured_annotations = StructuredAnnotations()

    out_xml_annotation_net = dict_to_xml_annotation(annot)
    ome.structured_annotations.append(out_xml_annotation_net)
    out = ome_types.to_xml(ome)
    print("Merged structured annotations")
    return out

def save_output(output: str,
                cost: float,
                attempts: float,
                pred_time: float,
                path:str,
                ) -> bool:
    """
    Save output to a file.

    Args:
        output (str): The output to save.
        path (str): The file path to save to.

    Returns:
        bool: True if save was successful, False otherwise.
    """
    try:
        with open(path, "w") as f:
            f.write(output)
            f.write("\nCOST: " + str(cost))
            f.write("\nATTEMPTS: " + str(attempts))
            f.write("\nTIME: " + str(pred_time))
        return True
    except IOError as e:
        print(f"Error saving output: {e}")
        return False

def load_output(path: str
                ) -> tuple[Optional[str], Optional[float]]:
    """
    Load output from a file.

    Args:
        path (str): The file path to load from.

    Returns:
        Optional[str]: The loaded output, or None if an error occurred.
    """
    output, cost, attempts, pred_time = None, None, None, None
    with open(path, "r") as f:
        lines = f.readlines()
        output = ""
        for line in lines:
            if line.startswith("COST"):
                cost = safe_float(line.split(":")[-1].strip())
            elif line.startswith("ATTEMPTS"):
                attempts = safe_float(line.split(":")[-1].strip())
            elif line.startswith("TIME"):
                pred_time = safe_float(line.split(":")[-1].strip())
            else:
                output += line
    try:
        output = ast.literal_eval(output)
        merge_xml_annotation(annot=output)
    except Exception as e:
        output = to_xml(from_xml(output))

    return output, cost, attempts, pred_time
    
    
def make_prediction(predictor,
                    in_data,
                    dataset,
                    file_name:str,
                    index:int,
                    should_predict="maybe",
                    start_point=None,
                    data_format=None,
                    model:str=None,
                    out_path:str=None):
    """
    TODO: add docstring
    """
    method = predictor.__name__
    path = f"{out_path}/predictor_outputs/{method}_{file_name}.txt"
    if not os.path.exists(os.path.dirname(path)):
        os.makedirs(os.path.dirname(path))
    print("-"*10+method+"_"+file_name+"-"*10)
    out, cost, attempts, pred_time = None, None, None, None
    if should_predict == "maybe" or "no":
        try:
            print("Trying to load output from file")
            out, cost, attempts, pred_time = load_output(path)
            if out and isinstance(out, dict) and start_point:
                out = merge_xml_annotation(annot=out, ome=start_point)
        except Exception as e:
            print("there was an error when loading a the file", e)

    if should_predict == "yes" or (not out and should_predict != "no"):
            pred_obj = predictor(in_data)
            if model:
                pred_obj.model = model
            t0 = time.time()
            out, cost, attempts = pred_obj.predict()
            t1 = time.time()
            pred_time = t1-t0
            print(f"Predicted in {pred_time:.2f} seconds")

            # save the output to file
            if out and isinstance(out, dict) and start_point:
                out = merge_xml_annotation(annot=out, ome=start_point)

            print("Trying to save output to file")
            if out is None:
                out = to_xml(OME())
            out = to_xml(from_xml(out))
            save_output(out, cost, attempts, pred_time,path=path)
            print("Predicted and Saved output to file")
    
    if not out:
        return
    out_sample = Sample(file_name=file_name,
                        metadata_str=out,
                        method=camel_to_snake(method),
                        format=data_format,
                        cost=cost,
                        attempts=attempts,
                        index=index,
                        gpt_model=model,
                        time=pred_time,
                        name=f"{file_name}_{method}_{iter}")
     
    dataset.add_sample(out_sample)

def flatten(container):
    for i in container:
        if isinstance(i, (list, tuple, set)):
            yield from flatten(i)
        else:
            yield i


def read_ome_xml(
        path: str
        )-> ET.Element:
    """
    This method reads the ome xml file and returns the ET root element.
    """
    tree = ET.parse(path)
    root = tree.getroot()
    print("root", tree)
    return root

def get_json(
        xml_root: ET.Element,
        paths={}
        ) -> dict:
    """
    Helper function to get all paths in an XML tree.
    :return: set of paths
    """
    for child in xml_root:
        child_tag = child.tag.split('}')[1]
        if child.attrib:
            new_dict = {}
            new_dict[child_tag] = {}

            for key in child.attrib.keys():
                new_dict[child_tag][key] = child.attrib[key]

            if child_tag+"s" not in paths:
                paths[child_tag+"s"] = []
            paths[child_tag+"s"] += [new_dict]

            get_json(child, new_dict[child_tag])
        elif child.text:
            paths[child_tag] = child.text
        else:
            new_path = {}
            paths[child_tag] = new_path
            get_json(child, new_path)
    return paths

def generate_paths(
        json_data:Union[dict, list],
        current_path:str="",
        paths:list=None
        ) -> list:
    """
    Generate all possible paths from a nested JSON structure.

    This function traverses a nested JSON structure (which may contain dictionaries and lists)
    and generates a list of all possible paths within it. For dictionaries, it uses the keys
    to build the path. For lists, it uses indices, except when a list item is a dictionary
    with an 'id' key, in which case it uses the id value in the path.

    Args:
        json_data (dict or list): The nested JSON structure to traverse.
        current_path (str, optional): The current path being built. Used in recursive calls.
                                      Defaults to an empty string.
        paths (list, optional): The list to store all generated paths. Used in recursive calls.
                                Defaults to None, which initializes an empty list.

    Returns:
        list: A list of strings, where each string represents a path in the format
              "path/to/element = value".

    Examples:
        >>> json_data = {
        ...     "test": 5,
        ...     "images": [
        ...         {"image": {"id": "image:0"}},
        ...         {"image": {"id": "image:1"}}
        ...     ],
        ...     "nested": {
        ...         "key": "value",
        ...         "list": [1, 2, 3]
        ...     }
        ... }
        >>> result = generate_paths(json_data)
        >>> for path in result:
        ...     print(path)
        test = 5
        images/0/image/id = image:0
        images/1/image/id = image:1
        nested/key = value
        nested/list/0 = 1
        nested/list/1 = 2
        nested/list/2 = 3
    """
    if paths is None:
        paths = []

    if isinstance(json_data, dict):
        for key, value in json_data.items():
            new_path = f"{current_path}/{key}" if current_path else key
            
            if isinstance(value, (dict, list)):
                generate_paths(value, new_path, paths)
            else:
                paths.append(f"{new_path} = {value}")

    elif isinstance(json_data, list):
        for index, item in enumerate(json_data):
            if isinstance(item, dict) and 'id' in item:
                new_path = f"{current_path}/{item['id'].split(':')[1]}"
                generate_paths(item, new_path, paths)
            else:
                new_path = f"{current_path}/{index}"
                if isinstance(item, (dict, list)):
                    generate_paths(item, new_path, paths)
                else:
                    paths.append(f"{new_path} = {item}")

    return paths

def safe_float(value):
    """
    Safely convert a value to float, returning None if conversion is not possible.

    Args:
        value: The value to convert to float.

    Returns:
        float or None: The float value if conversion is successful, None otherwise.
    """
    try:
        return float(value)
    except (ValueError, TypeError):
        return None
    

def ensure_path_exists(data: Dict[str, Any], path: str) -> None:
    """
    Ensure that the path exists in the data structure, creating empty lists or dicts as needed.
    """
    parts = path.strip("/").split("/").remove("") or []
    current = data
    for i, part in enumerate(parts):
        if part == "-" or part.isdigit():
            if not isinstance(current, list):
                current = []
            if part == "-" or int(part) == len(current):
                current.append({})
            elif int(part) > len(current):
                current.extend([{} for _ in range(int(part) - len(current) + 1)])
            current = current[int(part) if part != "-" else -1]
        else:
            if part not in current:
                if i < len(parts) - 1 and (parts[i+1] == "-" or parts[i+1].isdigit()):
                    current[part] = []
                else:
                    current[part] = {}
            current = current[part]

def custom_apply(patch: jsonpatch.JsonPatch, data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Apply the JSON Patch, automatically creating missing nodes.
    """
    for operation in patch:
        if operation["op"] in ["add", "replace"]:
            ensure_path_exists(data, "/".join(operation["path"].split("/")[:-1]))
        elif operation["op"] == "remove":
            ensure_path_exists(data, operation["path"])
    return patch.apply(data)

def update_state(current_state: OME, proposed_change: list) -> OME:
    """
    Update the OME state based on proposed changes using JSONPatch, automatically creating missing nodes.

    Args:
        current_state (OME): The current OME state.
        proposed_change (list): The change proposed as a JSON Patch document.

    Returns:
        OME: The updated OME state.

    Raises:
        jsonpatch.JsonPatchException: If the patch is invalid or cannot be applied.
        ValueError: If the resulting document is not a valid OME model.
    """
    # Convert current state to a dictionary, ensuring all default empty lists are included
    current_dict = json.loads(current_state.model_dump_json())

    try:
        # Apply the JSON Patch with custom logic to create missing nodes
        patch = jsonpatch.JsonPatch(proposed_change)
        updated_dict = custom_apply(patch, current_dict)

        # Convert the updated dictionary back to an OME object
        updated_state = from_dict(updated_dict, state=current_state)

        return updated_state

    except jsonpatch.JsonPatchException as e:
        raise ValueError(f"Invalid JSON Patch: {str(e)}")
    except Exception as e:
        raise ValueError(f"Error applying patch or converting to OME: {str(e)}")
    

def browse_schema(cls: BaseModel, additional_ignored_keywords: List[str] = [], max_depth: int = float('inf')) -> Dict[str, Any]:
    """
    Browse a schema as jsonschema, with depth control.

    Args:
        cls (BaseModel): The Pydantic model to convert to a schema.
        additional_ignored_keywords (List[str], optional): Additional keywords to ignore in the schema. Defaults to [].
        max_depth (int, optional): Maximum depth of nesting to include in the schema. Defaults to infinity.

    Returns:
        dict: A dictionary in the format of OpenAI's schema as jsonschema
    """
    schema = cls.model_json_schema()
    docstring = parse(cls.__doc__ or "")
    
    def clean_description(desc: Union[str, Any]) -> str:
        if not isinstance(desc, str):
            return str(desc)
        cleaned = re.sub(r'\s+', ' ', desc)
        return cleaned.strip()

    def resolve_ref(ref: str, definitions: Dict[str, Any]) -> Dict[str, Any]:
        if not ref.startswith('#/$defs/'):
            return {'type': 'object', 'description': f'Unresolved reference: {ref}'}
        def_name = ref.split('/')[-1]
        return definitions.get(def_name, {'type': 'object', 'description': f'Missing definition: {def_name}'})

    def flatten_schema(prop: Any, definitions: Dict[str, Any], current_depth: int = 0) -> Any:
        if not isinstance(prop, dict) or current_depth >= max_depth:
            return {'type': 'object', 'description': 'Nested object (depth limit reached)'} if isinstance(prop, dict) else prop

        flattened = {}
        unnecessary_keys = [] + additional_ignored_keywords
        for key, value in prop.items():
            if key not in unnecessary_keys:
                if key == '$ref':
                    if current_depth < max_depth:
                        flattened.update(flatten_schema(resolve_ref(value, definitions), definitions, current_depth + 1))
                    else:
                        flattened['type'] = 'object'
                        flattened['description'] = 'Nested object (depth limit reached)'
                elif key == "required" and (value == True or value == False):
                    continue
                elif key == 'description' and type(value) == str:
                    flattened[key] = clean_description(value)
                elif isinstance(value, dict):
                    if current_depth < max_depth:
                        flattened[key] = flatten_schema(value, definitions, current_depth + 1)
                    else:
                        flattened[key] = {'type': 'object', 'description': 'Nested object (depth limit reached)'}
                elif isinstance(value, list):
                    if current_depth < max_depth:
                        flattened[key] = [flatten_schema(item, definitions, current_depth + 1) for item in value]
                    else:
                        flattened[key] = [{'type': 'object', 'description': 'Nested object (depth limit reached)'}]
                else:
                    flattened[key] = value

        return flattened

    # Extract $defs if present
    definitions = schema.pop('$defs', {})

    # Flatten the main schema
    flattened_schema = flatten_schema(schema, definitions)

    # Add flattened definitions to properties
    if 'properties' not in flattened_schema:
        flattened_schema['properties'] = {}
    flattened_schema['properties']['definitions'] = {
        'type': 'object',
        'properties': {k: flatten_schema(v, definitions) for k, v in definitions.items()}
    }

    # Add descriptions from docstring
    for param in docstring.params:
        if param.arg_name in flattened_schema.get('properties', {}):
            if isinstance(flattened_schema['properties'][param.arg_name], dict):
                flattened_schema['properties'][param.arg_name]['description'] = clean_description(param.description)
            else:
                # If it's not a dict (due to max_depth), we replace it with a dict containing the description
                flattened_schema['properties'][param.arg_name] = {
                    'type': 'object',
                    'description': clean_description(param.description) + ' (Nested object, depth limit reached)'
                }

    # Combine short_description and long_description for a more complete description
    full_description = docstring.short_description or ""
    if docstring.long_description:
        full_description += " " + docstring.long_description if full_description else docstring.long_description
    
    description = clean_description(full_description) if full_description else f"Correctly extracted `{cls.__name__}` with all the required parameters with correct types"

    return {
        "name": schema.get('title', cls.__name__),
        "description": description,
        "parameters": {
            "type": "object",
            "required": [],
            "properties": flattened_schema["properties"]
        }
    }

def num_tokens_from_string(string: str, encoding_name: str="cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def camel_to_snake(name):
    """
    Convert a CamelCase string to snake_case.

    Parameters:
    name (str): The CamelCase string to convert.

    Returns:
    str: The converted snake_case string.
    """
    # Insert an underscore before each uppercase letter (except the first one) and convert to lowercase
    snake_case_name = re.sub(r'(?<!^)(?=[A-Z])', '_', name)
    return snake_case_name
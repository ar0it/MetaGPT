# 03.07.24
- build the analysis pipeline for the struc annotation track and run it 
- the raw data which is read by bioformats still seam to not be the correct one
- network of for struc annotations
- tree swarm for struc annotations

# 02.07.4
- Structured Annotations track:
    - try to improve the one shot results via prompt enigneering.
    - try the predictor tree, with structured annotations as root
    - try a multi step predictor which first isolatest non OME data and then creates the structured annotation

# 28.06.24
- Structured annotation track:
    - 

# 27.06.24
- General Things:
    - Dont spend more than 500â‚¬ for all things combined
- Structured annotation track:
    - no validation required here (maybe )
    - start with single marvin agents with empty state
    - continue with network approach (first sort data in data which is not present in autogenerated OME XML)
    - ?? do the fancy graph approach?

- Qualitative Results track:
    - show the results for the singular agent on marvin agent + ome schema for very small test metadata
    - ... on network for very small test metadata
    - show the results for the very small test metadata for the graph approach
    

- Quantitative Results track:

# 20.06.24
- the state incorporate the lower lvvl states which have alrdy been predicted, but the llm doesnt know that and tries to predict those again maybe fixed by deleting added raw_meta, also need to restructure prompt probably
- use the same assistant/ thread for all predictions?
- MaybeModels create problems since the subnodes are not added currently
- Model requiring another object for instantiation might be only relevant for image/pixels 

# 19.06.24
- what to do about nodes that can occur multiple times (in the tree model)
- Remove metadata already implemented from raw_meta ?
- unrecognized fields

# 16.06.24
- Limit the tokens used for a request. (I want control over the cost)
- Implement code async?

# 14.06.24

**ToDos**
- Maybe develop a heruistic on how to hand the metadata to the agent?
    - ideas:
    - sort by key length (proportional to nestedness)
        - alsways provide ~ 10 keys at once

    - sort by count of "|" as this is a common delimiter for the path in keys (proportional for nestedness)
    - have an llm one shot the order?
    - try to build a graph from the "|" structure

- maybe give the proprietary file format as input aswell
- maybe compress error messages handed to gpt4o via a gpt3.5 instance
- funny idea for are llms intelligent --> give a base task i.e. write a text. then recursively simply ask "improve this" do the same with humans. How does the curve look like for a plot where x is the iterations and y is the quality of the response.

- Idea for swarm: 1 agent doesn the schema focused retrieval, 1 agent who does the raw data retrieval, 1 Agent who asks questions and 1 agent who does the tool usage.
- Question answering seems to limit the scope, and might be more reliable than free responses
- Structure the OME schema into a questionair which the ai models chats with.
# 13.06.24

**Known Bugs/ToDos remaining:**

---

# 12.06.24

**Known Bugs/Todos remaining:**
- ~~Fix bugs with the reading of the raw metadata~~
- ~~provide a starting state to marvin~~
- iteratively update the state to validate more regularly (need more concrete feedback)
- generate some nicer figures, approach the figure business more systematically
- create some red line for the thesis, which questions need to be answered

**Thoughts**
- There are some metadata foramts whihc encrypt their data to sth like unit: 8
where 8 is by definition mm. This will be hard for an LLM to know
---